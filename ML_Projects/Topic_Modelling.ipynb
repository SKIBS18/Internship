{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\punee\\anaconda3\\lib\\site-packages (3.8.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.15.5)\n",
      "Requirement already satisfied, skipping upgrade: boto in c:\\users\\punee\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\punee\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.19.0,>=1.18.5 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in c:\\users\\punee\\anaconda3\\lib\\site-packages (from botocore<1.19.0,>=1.18.5->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(r'C://Users//punee//Corona_NLP_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>3794</td>\n",
       "      <td>48746</td>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>3795</td>\n",
       "      <td>48747</td>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>3796</td>\n",
       "      <td>48748</td>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>3797</td>\n",
       "      <td>48749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>3798</td>\n",
       "      <td>48750</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserName  ScreenName             Location     TweetAt  \\\n",
       "0            1       44953                  NYC  02-03-2020   \n",
       "1            2       44954          Seattle, WA  02-03-2020   \n",
       "2            3       44955                  NaN  02-03-2020   \n",
       "3            4       44956          Chicagoland  02-03-2020   \n",
       "4            5       44957  Melbourne, Victoria  03-03-2020   \n",
       "...        ...         ...                  ...         ...   \n",
       "3793      3794       48746            Israel ??  16-03-2020   \n",
       "3794      3795       48747       Farmington, NM  16-03-2020   \n",
       "3795      3796       48748        Haverford, PA  16-03-2020   \n",
       "3796      3797       48749                  NaN  16-03-2020   \n",
       "3797      3798       48750  Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          OriginalTweet           Sentiment  \n",
       "0     TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1     When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2     Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3     #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4     #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  \n",
       "...                                                 ...                 ...  \n",
       "3793  Meanwhile In A Supermarket in Israel -- People...            Positive  \n",
       "3794  Did you panic buy a lot of non-perishable item...            Negative  \n",
       "3795  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral  \n",
       "3796  Gov need to do somethings instead of biar je r...  Extremely Negative  \n",
       "3797  I and @ForestandPaper members are committed to...  Extremely Positive  \n",
       "\n",
       "[3798 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data[['OriginalTweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\punee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "text['index'] = text.index\n",
    "documents = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  index\n",
       "0  TRENDING: New Yorkers encounter empty supermar...      0\n",
       "1  When I couldn't find hand sanitizer at Fred Me...      1\n",
       "2  Find out how you can protect yourself and love...      2\n",
       "3  #Panic buying hits #NewYork City as anxious sh...      3\n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...      4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('watering', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jwelled</td>\n",
       "      <td>jwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3        jwelled   jwell\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'jwelled', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['When', 'I', \"couldn't\", 'find', 'hand', 'sanitizer', 'at', 'Fred', 'Meyer,', 'I', 'turned', 'to', '#Amazon.', 'But', '$114.97', 'for', 'a', '2', 'pack', 'of', 'Purell??!!Check', 'out', 'how', '', '#coronavirus', 'concerns', 'are', 'driving', 'up', 'prices.', 'https://t.co/ygbipBflMY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['couldn', 'hand', 'sanit', 'fred', 'meyer', 'turn', 'amazon', 'pack', 'purel', 'check', 'coronavirus', 'concern', 'drive', 'price', 'https', 'ygbipbflmi']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 1].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = documents['OriginalTweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [trend, yorker, encount, supermarket, shelv, p...\n",
       "1       [couldn, hand, sanit, fred, meyer, turn, amazo...\n",
       "2                       [protect, love, one, coronavirus]\n",
       "3       [panic, buy, hit, newyork, citi, anxious, shop...\n",
       "4       [toiletpap, dunnypap, coronavirus, covid_, new...\n",
       "                              ...                        \n",
       "3793    [supermarket, israel, peopl, danc, sing, stay,...\n",
       "3794    [panic, perish, item, echo, need, food, donat,...\n",
       "3795    [asst, prof, econom, cconc, nbcphiladelphia, t...\n",
       "3796    [need, someth, instead, biar, rakyat, assum, l...\n",
       "3797    [forestandpap, member, commit, safeti, employe...\n",
       "Name: OriginalTweet, Length: 3798, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 brooklyn\n",
      "1 coronavirus\n",
      "2 encount\n",
      "3 fear\n",
      "4 foodkick\n",
      "5 grocer\n",
      "6 https\n",
      "7 ivmkmsqdt\n",
      "8 maxdeliveri\n",
      "9 onlin\n",
      "10 pcrlwh\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(636 unique tokens: ['fear', 'pictur', 'sell', 'shelv', 'shopper']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]\n",
    "bow_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 5 (\"amazon\") appears 1 time.\n",
      "Word 6 (\"check\") appears 1 time.\n",
      "Word 7 (\"concern\") appears 1 time.\n",
      "Word 8 (\"couldn\") appears 1 time.\n",
      "Word 9 (\"drive\") appears 1 time.\n",
      "Word 10 (\"hand\") appears 1 time.\n",
      "Word 11 (\"pack\") appears 1 time.\n",
      "Word 12 (\"sanit\") appears 1 time.\n",
      "Word 13 (\"turn\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[1]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4137498891773347),\n",
      " (1, 0.5862549915322198),\n",
      " (2, 0.3964646133100739),\n",
      " (3, 0.3118514270752582),\n",
      " (4, 0.48029221514960924)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\" + 0.013*\"buy\" + 0.010*\"crazi\" + 0.010*\"retail\" + 0.010*\"thing\" + 0.010*\"want\"\n",
      "Topic: 1 \n",
      "Words: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\" + 0.011*\"long\" + 0.010*\"week\" + 0.010*\"time\" + 0.010*\"suppli\" + 0.009*\"line\"\n",
      "Topic: 2 \n",
      "Words: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\" + 0.017*\"buy\" + 0.016*\"shelv\" + 0.013*\"bank\" + 0.013*\"item\" + 0.013*\"donat\"\n",
      "Topic: 3 \n",
      "Words: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\" + 0.015*\"mask\" + 0.013*\"today\" + 0.013*\"suppli\" + 0.012*\"sanit\" + 0.011*\"retail\"\n",
      "Topic: 4 \n",
      "Words: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\" + 0.011*\"help\" + 0.011*\"drop\" + 0.011*\"increas\" + 0.010*\"think\" + 0.010*\"coronaoutbreak\"\n",
      "Topic: 5 \n",
      "Words: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\" + 0.016*\"coronapocalyps\" + 0.013*\"home\" + 0.008*\"suppli\" + 0.008*\"work\" + 0.008*\"lockdown\"\n",
      "Topic: 6 \n",
      "Words: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\" + 0.013*\"time\" + 0.012*\"pay\" + 0.012*\"corona\" + 0.011*\"virus\" + 0.011*\"famili\"\n",
      "Topic: 7 \n",
      "Words: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\" + 0.012*\"leav\" + 0.011*\"test\" + 0.011*\"think\" + 0.010*\"custom\" + 0.010*\"market\"\n",
      "Topic: 8 \n",
      "Words: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\" + 0.013*\"like\" + 0.013*\"product\" + 0.013*\"prepar\" + 0.012*\"week\" + 0.011*\"worri\"\n",
      "Topic: 9 \n",
      "Words: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\" + 0.013*\"local\" + 0.013*\"paper\" + 0.011*\"countri\" + 0.010*\"know\" + 0.010*\"elder\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.014*\"local\" + 0.013*\"hand\" + 0.012*\"like\" + 0.012*\"today\" + 0.010*\"sanit\" + 0.009*\"paper\" + 0.008*\"busi\" + 0.008*\"toilet\" + 0.008*\"worri\" + 0.008*\"suppli\"\n",
      "Topic: 1 Word: 0.014*\"retail\" + 0.011*\"busi\" + 0.011*\"right\" + 0.010*\"shelv\" + 0.010*\"consum\" + 0.008*\"year\" + 0.008*\"staff\" + 0.007*\"think\" + 0.007*\"want\" + 0.007*\"work\"\n",
      "Topic: 2 Word: 0.013*\"mask\" + 0.012*\"like\" + 0.011*\"help\" + 0.009*\"look\" + 0.008*\"virus\" + 0.008*\"coronaoutbreak\" + 0.008*\"face\" + 0.008*\"local\" + 0.008*\"corona\" + 0.007*\"elder\"\n",
      "Topic: 3 Word: 0.010*\"panicbuy\" + 0.009*\"coronapocalyps\" + 0.009*\"know\" + 0.009*\"care\" + 0.008*\"like\" + 0.008*\"work\" + 0.008*\"coronaoutbreak\" + 0.008*\"consum\" + 0.007*\"stay\" + 0.007*\"time\"\n",
      "Topic: 4 Word: 0.012*\"coronaoutbreak\" + 0.011*\"paper\" + 0.011*\"toilet\" + 0.009*\"suppli\" + 0.009*\"close\" + 0.009*\"quarantin\" + 0.008*\"like\" + 0.008*\"shelv\" + 0.007*\"consum\" + 0.007*\"water\"\n",
      "Topic: 5 Word: 0.012*\"paper\" + 0.012*\"toilet\" + 0.012*\"like\" + 0.011*\"buy\" + 0.011*\"come\" + 0.011*\"week\" + 0.010*\"virus\" + 0.010*\"today\" + 0.009*\"corona\" + 0.009*\"home\"\n",
      "Topic: 6 Word: 0.010*\"work\" + 0.009*\"worker\" + 0.009*\"retail\" + 0.009*\"demand\" + 0.008*\"pandem\" + 0.007*\"surviv\" + 0.007*\"thank\" + 0.007*\"time\" + 0.007*\"paper\" + 0.007*\"toilet\"\n",
      "Topic: 7 Word: 0.015*\"buy\" + 0.011*\"stop\" + 0.008*\"time\" + 0.008*\"fear\" + 0.008*\"amazon\" + 0.008*\"start\" + 0.007*\"actual\" + 0.007*\"fresh\" + 0.007*\"say\" + 0.007*\"work\"\n",
      "Topic: 8 Word: 0.011*\"leav\" + 0.010*\"think\" + 0.009*\"good\" + 0.009*\"line\" + 0.009*\"coronapocalyps\" + 0.009*\"pile\" + 0.009*\"work\" + 0.009*\"toilet\" + 0.009*\"time\" + 0.008*\"paper\"\n",
      "Topic: 9 Word: 0.010*\"time\" + 0.009*\"buy\" + 0.008*\"toilet\" + 0.008*\"paper\" + 0.008*\"day\" + 0.007*\"retail\" + 0.007*\"canada\" + 0.007*\"shelv\" + 0.007*\"local\" + 0.007*\"help\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['protect', 'love', 'one', 'coronavirus']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.774975061416626\t \n",
      "Topic: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\" + 0.016*\"coronapocalyps\" + 0.013*\"home\" + 0.008*\"suppli\" + 0.008*\"work\" + 0.008*\"lockdown\"\n",
      "\n",
      "Score: 0.025005266070365906\t \n",
      "Topic: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\" + 0.013*\"local\" + 0.013*\"paper\" + 0.011*\"countri\" + 0.010*\"know\" + 0.010*\"elder\"\n",
      "\n",
      "Score: 0.025003457441926003\t \n",
      "Topic: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\" + 0.012*\"leav\" + 0.011*\"test\" + 0.011*\"think\" + 0.010*\"custom\" + 0.010*\"market\"\n",
      "\n",
      "Score: 0.025003179907798767\t \n",
      "Topic: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\" + 0.013*\"buy\" + 0.010*\"crazi\" + 0.010*\"retail\" + 0.010*\"thing\" + 0.010*\"want\"\n",
      "\n",
      "Score: 0.0250031016767025\t \n",
      "Topic: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\" + 0.011*\"help\" + 0.011*\"drop\" + 0.011*\"increas\" + 0.010*\"think\" + 0.010*\"coronaoutbreak\"\n",
      "\n",
      "Score: 0.025002548471093178\t \n",
      "Topic: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\" + 0.013*\"time\" + 0.012*\"pay\" + 0.012*\"corona\" + 0.011*\"virus\" + 0.011*\"famili\"\n",
      "\n",
      "Score: 0.025002365931868553\t \n",
      "Topic: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\" + 0.015*\"mask\" + 0.013*\"today\" + 0.013*\"suppli\" + 0.012*\"sanit\" + 0.011*\"retail\"\n",
      "\n",
      "Score: 0.025001972913742065\t \n",
      "Topic: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\" + 0.017*\"buy\" + 0.016*\"shelv\" + 0.013*\"bank\" + 0.013*\"item\" + 0.013*\"donat\"\n",
      "\n",
      "Score: 0.02500169165432453\t \n",
      "Topic: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\" + 0.011*\"long\" + 0.010*\"week\" + 0.010*\"time\" + 0.010*\"suppli\" + 0.009*\"line\"\n",
      "\n",
      "Score: 0.02500133402645588\t \n",
      "Topic: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\" + 0.013*\"like\" + 0.013*\"product\" + 0.013*\"prepar\" + 0.012*\"week\" + 0.011*\"worri\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF modelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7749655246734619\t \n",
      "Topic: 0.010*\"panicbuy\" + 0.009*\"coronapocalyps\" + 0.009*\"know\" + 0.009*\"care\" + 0.008*\"like\" + 0.008*\"work\" + 0.008*\"coronaoutbreak\" + 0.008*\"consum\" + 0.007*\"stay\" + 0.007*\"time\"\n",
      "\n",
      "Score: 0.02500942163169384\t \n",
      "Topic: 0.010*\"work\" + 0.009*\"worker\" + 0.009*\"retail\" + 0.009*\"demand\" + 0.008*\"pandem\" + 0.007*\"surviv\" + 0.007*\"thank\" + 0.007*\"time\" + 0.007*\"paper\" + 0.007*\"toilet\"\n",
      "\n",
      "Score: 0.025004306808114052\t \n",
      "Topic: 0.013*\"mask\" + 0.012*\"like\" + 0.011*\"help\" + 0.009*\"look\" + 0.008*\"virus\" + 0.008*\"coronaoutbreak\" + 0.008*\"face\" + 0.008*\"local\" + 0.008*\"corona\" + 0.007*\"elder\"\n",
      "\n",
      "Score: 0.02500377781689167\t \n",
      "Topic: 0.012*\"paper\" + 0.012*\"toilet\" + 0.012*\"like\" + 0.011*\"buy\" + 0.011*\"come\" + 0.011*\"week\" + 0.010*\"virus\" + 0.010*\"today\" + 0.009*\"corona\" + 0.009*\"home\"\n",
      "\n",
      "Score: 0.025003759190440178\t \n",
      "Topic: 0.015*\"buy\" + 0.011*\"stop\" + 0.008*\"time\" + 0.008*\"fear\" + 0.008*\"amazon\" + 0.008*\"start\" + 0.007*\"actual\" + 0.007*\"fresh\" + 0.007*\"say\" + 0.007*\"work\"\n",
      "\n",
      "Score: 0.025003131479024887\t \n",
      "Topic: 0.010*\"time\" + 0.009*\"buy\" + 0.008*\"toilet\" + 0.008*\"paper\" + 0.008*\"day\" + 0.007*\"retail\" + 0.007*\"canada\" + 0.007*\"shelv\" + 0.007*\"local\" + 0.007*\"help\"\n",
      "\n",
      "Score: 0.02500269003212452\t \n",
      "Topic: 0.011*\"leav\" + 0.010*\"think\" + 0.009*\"good\" + 0.009*\"line\" + 0.009*\"coronapocalyps\" + 0.009*\"pile\" + 0.009*\"work\" + 0.009*\"toilet\" + 0.009*\"time\" + 0.008*\"paper\"\n",
      "\n",
      "Score: 0.025002652779221535\t \n",
      "Topic: 0.012*\"coronaoutbreak\" + 0.011*\"paper\" + 0.011*\"toilet\" + 0.009*\"suppli\" + 0.009*\"close\" + 0.009*\"quarantin\" + 0.008*\"like\" + 0.008*\"shelv\" + 0.007*\"consum\" + 0.007*\"water\"\n",
      "\n",
      "Score: 0.02500259131193161\t \n",
      "Topic: 0.014*\"local\" + 0.013*\"hand\" + 0.012*\"like\" + 0.012*\"today\" + 0.010*\"sanit\" + 0.009*\"paper\" + 0.008*\"busi\" + 0.008*\"toilet\" + 0.008*\"worri\" + 0.008*\"suppli\"\n",
      "\n",
      "Score: 0.0250021293759346\t \n",
      "Topic: 0.014*\"retail\" + 0.011*\"busi\" + 0.011*\"right\" + 0.010*\"shelv\" + 0.010*\"consum\" + 0.008*\"year\" + 0.008*\"staff\" + 0.007*\"think\" + 0.007*\"want\" + 0.007*\"work\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[2]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.47229981422424316\t Topic: 0.021*\"buy\" + 0.018*\"paper\" + 0.017*\"toilet\" + 0.016*\"suppli\" + 0.014*\"stop\"\n",
      "Score: 0.4276640713214874\t Topic: 0.026*\"work\" + 0.024*\"like\" + 0.017*\"home\" + 0.015*\"stay\" + 0.015*\"water\"\n",
      "Score: 0.012506397441029549\t Topic: 0.020*\"help\" + 0.018*\"buy\" + 0.018*\"time\" + 0.018*\"come\" + 0.016*\"coronaoutbreak\"\n",
      "Score: 0.012505966238677502\t Topic: 0.030*\"hand\" + 0.019*\"help\" + 0.016*\"like\" + 0.014*\"sanit\" + 0.013*\"toilet\"\n",
      "Score: 0.012505476363003254\t Topic: 0.017*\"local\" + 0.017*\"paper\" + 0.015*\"toilet\" + 0.013*\"work\" + 0.012*\"week\"\n",
      "Score: 0.01250457763671875\t Topic: 0.022*\"demand\" + 0.018*\"suppli\" + 0.013*\"servic\" + 0.012*\"work\" + 0.011*\"market\"\n",
      "Score: 0.012503928504884243\t Topic: 0.030*\"local\" + 0.027*\"paper\" + 0.027*\"time\" + 0.026*\"toilet\" + 0.022*\"like\"\n",
      "Score: 0.012503789737820625\t Topic: 0.025*\"retail\" + 0.020*\"close\" + 0.013*\"home\" + 0.011*\"demand\" + 0.011*\"school\"\n",
      "Score: 0.012502999044954777\t Topic: 0.033*\"toilet\" + 0.032*\"paper\" + 0.021*\"buy\" + 0.018*\"hand\" + 0.016*\"pandem\"\n",
      "Score: 0.012502964586019516\t Topic: 0.021*\"consum\" + 0.020*\"retail\" + 0.015*\"work\" + 0.015*\"leav\" + 0.014*\"test\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document='LetÃ‚â€™s ALL look after the less capable in our village and ensure they stay healthy. Bringing shopping to their doors, help with online '\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
